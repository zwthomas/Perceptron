/*
 * Zachary Thomas
 * 2/6/2018
 * Perceptron network to evaluate binary expressions
 */

#include "Network.h"
#include <iostream>
#include <unistd.h>

using namespace std;


/**
 * Creates a network of perceptrons of the input specifications. Each perceptron will have random weights.
 *
 * @param numLayers     The number of layers of perceptrons
 * @param numNodes      Array containing the number of nodes in each layer
 * @param numInputs     Number of inputs going into the first layer
 */
Network::Network(int numLayers, int numNodes[], int numInputs) {
    NUM_LAYERS = numLayers;
    NUM_NODES = numNodes;
    NUM_INPUTS = numInputs;
    LAYERS = new list<Perceptron>[numLayers];
    for (int layerCount = 0; layerCount < numLayers; layerCount++) {                // Creates each layer
        for (int nodeCount = 0; nodeCount < numNodes[layerCount]; nodeCount++) {    // Creates each node in the layer
            if (layerCount == 0) {
                LAYERS[layerCount].push_back(Perceptron(numInputs));                // Number of inputs in first layer
                                                                                    // Depends on number of inputs
            } else {
                LAYERS[layerCount].push_back(Perceptron(numNodes[layerCount - 1])); // Rest of rows depend on the previous
            }
        }
    }
}

/**
 * Cleans pointers in network.
 */
Network::~Network() {
    delete[] LAYERS;
}

/**
 * Displays the weights in the network for debugging purposes.
 */
void Network::displayNetwork() {
    for (int layer = 0; layer < NUM_LAYERS; layer++) {
        cout << "Layer " << layer + 1<<  endl;
        auto it = LAYERS[layer].begin();
        while(it != LAYERS[layer].end()) {
            cout << "NODE: "<<  endl;
            it++->displayWeights();
        }
    }
}

/**
 * Feeds the input through the network. Adjusting the weights of each node until the correct output is achieved.
 *
 * @param inputs    Array containing all the test cases
 * @param outputs   Array containing output to all of those test cases
 * @param cases     Number of test cases
 * @param numInputs Number of input per test case
 * @param outputNum Number of outputs generated by the network
 * @param file      Filename to save the weights to
 */
void Network::feedForward(int **inputs, int **outputs, int cases, int numInputs, int outputNum, string file) {
    double lr = .1;
    list<int> *ins;
    list<int> *tempOuts;

    for (int row = 0; row < cases; row++) {                     // Passes in each row of input
        cout<< "HERE" << endl;
        ins = fillInputs(inputs[row], numInputs);
        for (int layer = 0; layer < NUM_LAYERS; layer++) {      // Feeds the input and results through all of the layers
            tempOuts = new list<int>;
            tempOuts->push_back(1);                             // Bias

            auto percep = LAYERS[layer].begin();                // Pass input to all nodes in the layer
            while(percep != LAYERS[layer].end()) {
                int eval = percep->eval(ins);
                tempOuts->push_back(eval);         // Save the outputs for the input to the next row or final output
                percep++;
            }
            delete(ins);
            ins = tempOuts;
        }
        if (!checkOutputs(ins, outputs[row], outputNum)) {      // If the output is wrong adjust weights and start over
            adjustLayers(outputs[row],ins, lr, outputNum);\
            displayNetwork();
            //sleep(2);
            //lr *= .9;
            row = -1;
        }
        delete(ins);
    }
}

/**
 * Takes this inputs in the array and puts them in a list.
 *
 * @param inputs    Array containing inputs
 * @param row       Row in the array we are looking at
 * @param numInputs Number of inputs in that row
 * @return  the inputs for that row in a list
 */
std::list<int> *Network::fillInputs(int *inputs, int numInputs) {
    auto *inList = new list<int>();
    inList->push_back(1);
    for (int col = 0; col < numInputs; col++) {
        inList->push_back(inputs[col]);
    }
    return inList;
}

/**
 * Checks to see if the generated output matches correct output.
 *
 * @param generatedOutputs  list of generated output
 * @param correctOutputs    array of correct output
 * @param numOutputs        Number of outputs
 * @return  true if outputs match
 */
bool Network::checkOutputs(std::list<int> *generatedOutputs, int *correctOutputs, int numOutputs) {
    auto it = generatedOutputs->begin();
    it++;                                           // Skip the bias
    for (int ndx = 0; ndx < numOutputs; ndx++) {
        if ((*it++) != correctOutputs[ndx]) {
            return false;
        }
    }
    return true;
}


/**
 * Adjusts the weights of all the nodes in the network.
 *
 * @param output                correct output
 * @param generatedOutputs      generated output
 * @param learningRate          Multiplier to the change in the weight
 * @param numOutput             number of output
 */
void Network::adjustLayers(int *output, std::list<int> *generatedOutputs, double learningRate, int numOutput) {
    for (int layer = 0; layer < NUM_LAYERS; layer++) {      // Goes through all Layers

        auto percep = LAYERS[layer].begin();                // Goes through each of the nodes in each layer
        while(percep != LAYERS[layer].end()) {
            percep->adjust(learningRate, numOutput, output, generatedOutputs);
            percep++;
        }

    }
}

/**
 * For testing purposes. Allows to set the weights of individual nodes.
 *
 * @param col   Layer the node is in
 * @param row   What node in the layer
 * @param w     Array of weight to change to
 */
void Network::setWeight(int col, int row, double *w) {
    if (col >= NUM_LAYERS) {                        // Make sure the layer exists
        return;
    }
    auto nodeIt = LAYERS[col].begin();              // Goes to the node
    for (int nodeCount = 0; nodeCount < row && nodeIt != LAYERS[col].end(); nodeCount++) {
        nodeIt++;
    }
    if (nodeIt != LAYERS[col].end()) {              // Makes sure node exists
        nodeIt->setWeights(w);
    }

}




